{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: AWS S3 & Sourcing Datasets\n",
    "Republish this open dataset in Amazon S3 and share with us a link.\n",
    "You may run into 403 Forbidden errors as you test accessing this data. There is a way to comply with the BLS data access policies and re-gain access to fetch this data programatically - we have included some hints as to how to do this at the bottom of this README in the Q/A section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required sub-steps to accomplisht this:\n",
    "1. Set up S3 bucket environmnet\n",
    "2. Be able to read and publish into S3 env. \n",
    "3. troubleshoot error\n",
    "\n",
    "- Key notes: This is DataLake design. Create a landing-zone where data will be uploaded and parsed under upload date. The goal is just to have a starting point to being bringing in data into s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = os.getenv('AWS_ACCESS_KEY')\n",
    "secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-02-ep-website\n",
      "aws-emr-resources-386175835981-us-east-2\n",
      "aws-logs-386175835981-us-east-2\n",
      "bigbatbucket\n",
      "canvas-bucket-iris-4123\n",
      "dbdatalocation\n",
      "ed-exp-cost-and-usage\n",
      "eddysfistbuck\n",
      "edwardplatagschoolcap\n",
      "eplatacapstonedata\n",
      "eplatacapstoneipynb\n",
      "qep-sports-betting-bucket\n",
      "rearc-datalake-bucket\n",
      "redditdatacollectionwemeta\n",
      "sagemaker-soln-ddf-js-2ruwg4-386175835981-us-east-1\n",
      "sagemaker-soln-ddf-js-2seloa-386175835981-us-east-1\n",
      "sagemaker-soln-ddf-js-2sf3s6-386175835981-us-east-1\n",
      "sagemaker-soln-ddf-js-44xdya-386175835981-us-east-1\n",
      "sagemaker-soln-documents-js-4htc2a-us-east-1-386175835981\n",
      "sagemaker-studio-386175835981-l4ayz3cscdq\n",
      "sagemaker-studio-386175835981-l9tzph12na\n",
      "sagemaker-studio-386175835981-zzqac2052o\n",
      "sagemaker-us-east-2-386175835981\n",
      "someonesbucket\n",
      "ss-discord-group-minecraft-bucket\n"
     ]
    }
   ],
   "source": [
    "response = s3_client.list_buckets()\n",
    "\n",
    "if 'Buckets' in response:\n",
    "    buckets = response['Buckets']\n",
    "    for bucket in buckets:\n",
    "        print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have access to the s3 bucekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: AWS S3 & Sourcing Datasets\n",
    "1. Republish [this open dataset](https://download.bls.gov/pub/time.series/pr/) in Amazon S3 and share with us a link.\n",
    "    - You may run into 403 Forbidden errors as you test accessing this data. There is a way to comply with the BLS data access policies and re-gain access to fetch this data programatically - we have included some hints as to how to do this at the bottom of this README in the Q/A section.\n",
    "2. Script this process so the files in the S3 bucket are kept in sync with the source when data on the website is updated, added, or deleted.\n",
    "    - Don't rely on hard coded names - the script should be able to handle added or removed files.\n",
    "    - Ensure the script doesn't upload the same file more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "\n",
    "# AWS credentials\n",
    "access_key = os.getenv('AWS_ACCESS_KEY')\n",
    "secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# S3 bucket and landing zone details\n",
    "bucket_name = 'rearc-datalake-bucket'\n",
    "landing_zone_prefix = 'landing-zone/'\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")\n",
    "\n",
    "# Check if the landing zone exists, create it if it doesn't\n",
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=landing_zone_prefix\n",
    ")\n",
    "\n",
    "if 'Contents' not in response:\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=landing_zone_prefix\n",
    "    )\n",
    "    print(f\"Landing zone '{landing_zone_prefix}' created in bucket '{bucket_name}'.\")\n",
    "\n",
    "# Fetch data from the provided link and upload to S3 landing zone\n",
    "url = 'https://download.bls.gov/pub/time.series/pr/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    files = response.text.split('\\n')\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            file_name = file.split('/')[-1]\n",
    "            s3_client.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=f\"{landing_zone_prefix}{file_name}\",\n",
    "                Body=requests.get(f\"{url}{file}\").content\n",
    "            )\n",
    "            print(f\"Uploaded '{file_name}' to landing zone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://download.bls.gov/pub/time.series/pr/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    files = response.text.split('\\n')\n",
    "    for file in files:\n",
    "        print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'update_date': '12/6/2023',\n",
       "  'update_time': '8:30 AM',\n",
       "  'size': '102',\n",
       "  'data': 'pr.class 9/13/2022  3:52 PM          562 pr.contacts 12/6/2023  8:30 AM      1477682 pr.data.0.Current 12/6/2023  8:30 AM      3101899 pr.data.1.AllData 12/6/2023  8:30 AM          176 pr.duration 12/6/2023  8:30 AM           40 pr.footnote 12/6/2023  8:30 AM          745 pr.measure  1/7/1994  2:53 PM          146 pr.period11/18/2011  3:05 PM           79 pr.seasonal 12/6/2023  8:30 AM          263 pr.sector 12/6/2023  8:30 AM        15657 pr.series11/17/2011  4:11 PM        18343 pr.txt',\n",
       "  'data_link': 'https://download.bls.gov/pub/time.series/pr/pr.class 9/13/2022  3:52 PM          562 pr.contacts 12/6/2023  8:30 AM      1477682 pr.data.0.Current 12/6/2023  8:30 AM      3101899 pr.data.1.AllData 12/6/2023  8:30 AM          176 pr.duration 12/6/2023  8:30 AM           40 pr.footnote 12/6/2023  8:30 AM          745 pr.measure  1/7/1994  2:53 PM          146 pr.period11/18/2011  3:05 PM           79 pr.seasonal 12/6/2023  8:30 AM          263 pr.sector 12/6/2023  8:30 AM        15657 pr.series11/17/2011  4:11 PM        18343 pr.txt'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "def fetch_front_page_data(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    # Fetch the content from the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data from {url} with status code {response.status_code}\")\n",
    "\n",
    "    # Parse the HTML to extract file details\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    file_data = []\n",
    "\n",
    "    # Regular expression to match the file details\n",
    "    file_info_pattern = re.compile(r'(\\d{1,2}/\\d{1,2}/\\d{4})\\s+(\\d{1,2}:\\d{2} [APM]{2})\\s+([\\d,]+) (.+)')\n",
    "\n",
    "    for line in soup.get_text().split('\\n'):\n",
    "        match = file_info_pattern.search(line)\n",
    "        if match:\n",
    "            data = {\n",
    "                'update_date': match.group(1),\n",
    "                'update_time': match.group(2),\n",
    "                'size': match.group(3),\n",
    "                'data': match.group(4),\n",
    "                'data_link': os.path.join(url, match.group(4))\n",
    "            }\n",
    "            file_data.append(data)\n",
    "\n",
    "    return file_data\n",
    "\n",
    "# Example usage\n",
    "front_page_data = fetch_front_page_data(\"https://download.bls.gov/pub/time.series/pr/\")\n",
    "front_page_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>update_date</th>\n",
       "      <th>update_time</th>\n",
       "      <th>size</th>\n",
       "      <th>data</th>\n",
       "      <th>data_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/6/2023</td>\n",
       "      <td>8:30 AM</td>\n",
       "      <td>102</td>\n",
       "      <td>pr.class 9/13/2022  3:52 PM          562 pr.co...</td>\n",
       "      <td>https://download.bls.gov/pub/time.series/pr/pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  update_date update_time size  \\\n",
       "0   12/6/2023     8:30 AM  102   \n",
       "\n",
       "                                                data  \\\n",
       "0  pr.class 9/13/2022  3:52 PM          562 pr.co...   \n",
       "\n",
       "                                           data_link  \n",
       "0  https://download.bls.gov/pub/time.series/pr/pr...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(front_page_data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/ with status code 503",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(file_data)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Fetching and displaying data in DataFrame format\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m front_page_data_df \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_front_page_data_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://download.bls.gov/pub/time.series/pr/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m front_page_data_df\u001b[38;5;241m.\u001b[39mhead()  \u001b[38;5;66;03m# Displaying first few rows for brevity\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mfetch_front_page_data_df\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Parse the HTML to extract file details\u001b[39;00m\n\u001b[1;32m     14\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Failed to fetch data from https://download.bls.gov/pub/time.series/pr/ with status code 503"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_front_page_data_df(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    # Fetch the content from the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data from {url} with status code {response.status_code}\")\n",
    "\n",
    "    # Parse the HTML to extract file details\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    file_data = []\n",
    "\n",
    "    # Extract each file entry\n",
    "    for line in soup.find_all('a'):\n",
    "        line_text = line.find_previous('br').next_sibling.strip()\n",
    "        if line_text:\n",
    "            parts = line_text.split()\n",
    "            if len(parts) >= 3:\n",
    "                update_date = parts[0]\n",
    "                update_time = parts[1] + ' ' + parts[2]\n",
    "                size = parts[3]\n",
    "                data = line.text\n",
    "                data_link = url + line['href']\n",
    "\n",
    "                file_info = {\n",
    "                    'update_date': update_date,\n",
    "                    'update_time': update_time,\n",
    "                    'size': size,\n",
    "                    'data': data,\n",
    "                    'data_link': data_link\n",
    "                }\n",
    "                file_data.append(file_info)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(file_data)\n",
    "\n",
    "# Fetching and displaying data in DataFrame format\n",
    "front_page_data_df = fetch_front_page_data_df(\"https://download.bls.gov/pub/time.series/pr/\")\n",
    "front_page_data_df.head()  # Displaying first few rows for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_front_page_data_df(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data from {url} with status code {response.status_code}\")\n",
    "\n",
    "    # Save the HTML content\n",
    "    today = datetime.today().strftime('%Y_%m_%d')\n",
    "    filename = f'downloaded_data/front_page_{today}.html'\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = {}  # replace this with your parsing logic\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/ with status code 503",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_front_page_data_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mfetch_front_page_data_df\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Save the HTML content\u001b[39;00m\n\u001b[1;32m     15\u001b[0m today \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Failed to fetch data from https://download.bls.gov/pub/time.series/pr/ with status code 503"
     ]
    }
   ],
   "source": [
    "df = fetch_front_page_data_df(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets move to local"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
