{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: AWS S3 & Sourcing Datasets\n",
    "Republish this open dataset in Amazon S3 and share with us a link.\n",
    "You may run into 403 Forbidden errors as you test accessing this data. There is a way to comply with the BLS data access policies and re-gain access to fetch this data programatically - we have included some hints as to how to do this at the bottom of this README in the Q/A section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required sub-steps to accomplisht this:\n",
    "1. Set up S3 bucket environmnet\n",
    "2. Be able to read and publish into S3 env. \n",
    "3. troubleshoot error\n",
    "\n",
    "- Key notes: This is DataLake design. Create a landing-zone where data will be uploaded and parsed under upload date. The goal is just to have a starting point to being bringing in data into s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = os.getenv('AWS_ACCESS_KEY')\n",
    "secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-02-ep-website\n",
      "aws-emr-resources-386175835981-us-east-2\n",
      "aws-logs-386175835981-us-east-2\n",
      "bigbatbucket\n",
      "canvas-bucket-iris-4123\n",
      "dbdatalocation\n",
      "ed-exp-cost-and-usage\n",
      "eddysfistbuck\n",
      "edwardplatagschoolcap\n",
      "eplatacapstonedata\n",
      "eplatacapstoneipynb\n",
      "qep-sports-betting-bucket\n",
      "rearc-datalake-bucket\n",
      "redditdatacollectionwemeta\n",
      "sagemaker-soln-ddf-js-2ruwg4-386175835981-us-east-1\n",
      "sagemaker-soln-ddf-js-2seloa-386175835981-us-east-1\n",
      "sagemaker-soln-ddf-js-2sf3s6-386175835981-us-east-1\n",
      "sagemaker-soln-ddf-js-44xdya-386175835981-us-east-1\n",
      "sagemaker-soln-documents-js-4htc2a-us-east-1-386175835981\n",
      "sagemaker-studio-386175835981-l4ayz3cscdq\n",
      "sagemaker-studio-386175835981-l9tzph12na\n",
      "sagemaker-studio-386175835981-zzqac2052o\n",
      "sagemaker-us-east-2-386175835981\n",
      "someonesbucket\n",
      "ss-discord-group-minecraft-bucket\n"
     ]
    }
   ],
   "source": [
    "response = s3_client.list_buckets()\n",
    "\n",
    "if 'Buckets' in response:\n",
    "    buckets = response['Buckets']\n",
    "    for bucket in buckets:\n",
    "        print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have access to the s3 bucekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: AWS S3 & Sourcing Datasets\n",
    "1. Republish [this open dataset](https://download.bls.gov/pub/time.series/pr/) in Amazon S3 and share with us a link.\n",
    "    - You may run into 403 Forbidden errors as you test accessing this data. There is a way to comply with the BLS data access policies and re-gain access to fetch this data programatically - we have included some hints as to how to do this at the bottom of this README in the Q/A section.\n",
    "2. Script this process so the files in the S3 bucket are kept in sync with the source when data on the website is updated, added, or deleted.\n",
    "    - Don't rely on hard coded names - the script should be able to handle added or removed files.\n",
    "    - Ensure the script doesn't upload the same file more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.class\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.contacts\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.data.0.Current\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.data.1.AllData\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.duration\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.footnote\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.measure\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.period\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.seasonal\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.sector\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.series\n",
      "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/pr.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://download.bls.gov/pub/time.series/pr/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    files = response.text.split('\\n')\n",
    "    for file in files:\n",
    "        print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to fetch all the data from these sources:\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.class\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.contacts\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.data.0.Current\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.data.1.AllData\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.duration\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.footnote\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.measure\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.period\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.seasonal\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.sector\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.series\n",
    "#https://download.bls.gov/pub/time.series/pr/pr.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to fetch data from\n",
    "urls = [\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.class\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.contacts\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.data.0.Current\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.data.1.AllData\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.duration\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.footnote\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.measure\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.period\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.seasonal\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.sector\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.series\",\n",
    "    \"https://download.bls.gov/pub/time.series/pr/pr.txt\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://download.bls.gov/pub/time.series/', 'https://download.bls.gov/pub/time.series/pr/pr.class', 'https://download.bls.gov/pub/time.series/pr/pr.contacts', 'https://download.bls.gov/pub/time.series/pr/pr.data.0.Current', 'https://download.bls.gov/pub/time.series/pr/pr.data.1.AllData', 'https://download.bls.gov/pub/time.series/pr/pr.duration', 'https://download.bls.gov/pub/time.series/pr/pr.footnote', 'https://download.bls.gov/pub/time.series/pr/pr.measure', 'https://download.bls.gov/pub/time.series/pr/pr.period', 'https://download.bls.gov/pub/time.series/pr/pr.seasonal', 'https://download.bls.gov/pub/time.series/pr/pr.sector', 'https://download.bls.gov/pub/time.series/pr/pr.series', 'https://download.bls.gov/pub/time.series/pr/pr.txt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_urls(base_url):\n",
    "    # Set the user agent header\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    # Send a GET request to the URL with the user agent header\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all the <a> tags in the HTML\n",
    "        links = soup.find_all('a')\n",
    "\n",
    "        # Extract the href attribute from each <a> tag\n",
    "        urls = [link.get('href') for link in links if link.get('href')]\n",
    "\n",
    "        # Prepend the base URL to each URL\n",
    "        urls = [urljoin(base_url, url) for url in urls]\n",
    "\n",
    "        return urls\n",
    "    else:\n",
    "        print(f\"Failed to fetch HTML from {base_url}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "base_url = 'https://download.bls.gov/pub/time.series/pr/'\n",
    "urls_list = generate_urls(base_url)\n",
    "print(urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_list == urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://download.bls.gov/pub/time.series/pr/pr.class'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets remove the first one, as that is the base url\n",
    "urls.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def fetch_data(urls):\n",
    "    data = {}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    for url in urls:\n",
    "        # Send a GET request to the URL with the user agent header\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Get the response content as text\n",
    "            page_source = response.text\n",
    "            data[url] = page_source\n",
    "        else:\n",
    "            print(f\"Failed to fetch data from {url}\")\n",
    "        \n",
    "        # Add a delay between requests\n",
    "        time.sleep(1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframes = {}\n",
    "\n",
    "for url in urls:\n",
    "    data = fetch_data([url])\n",
    "    dataframe = pd.DataFrame(data[url].split('\\n'))\n",
    "    dataframe = dataframe[0].str.split('\\t', expand=True)\n",
    "    dataframes[url] = dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0            1              2           3                4\n",
      "0  class_code   class_text  display_level  selectable  sort_sequence\\r\n",
      "1           3    Employees              0           T              2\\r\n",
      "2           6  All workers              0           T              1\\r\n",
      "3                     None           None        None             None\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(dataframes['https://download.bls.gov/pub/time.series/pr/pr.class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, dataframe in dataframes.items():\n",
    "    dataframe.columns = dataframe.iloc[0]\n",
    "    dataframe = dataframe[1:]\n",
    "    dataframes[url] = dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_dataframes_as_csv(dataframes):\n",
    "    # Create the \"data\" directory if it doesn't exist\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "\n",
    "    for url, dataframe in dataframes.items():\n",
    "        # Modify the dataset name for the CSV file\n",
    "        dataset_name = url.split(\"/\")[-1].replace(\".\", \"_\") + \".csv\"\n",
    "        # Save the dataframe as a CSV file in the \"data\" directory\n",
    "        dataframe.to_csv(os.path.join(\"data\", dataset_name), index=False)\n",
    "\n",
    "# Example usage\n",
    "save_dataframes_as_csv(dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: pr_series.csv\n",
      "   series_id          sector_code  class_code  measure_code  duration_code  \\\n",
      "0  PRS30006011             3000.0         6.0           1.0            1.0   \n",
      "1  PRS30006012             3000.0         6.0           1.0            2.0   \n",
      "2  PRS30006013             3000.0         6.0           1.0            3.0   \n",
      "3  PRS30006021             3000.0         6.0           2.0            1.0   \n",
      "4  PRS30006022             3000.0         6.0           2.0            2.0   \n",
      "\n",
      "  seasonal base_year  footnote_codes  begin_year begin_period  end_year  \\\n",
      "0        S         -             NaN      1988.0          Q01    2023.0   \n",
      "1        S         -             NaN      1987.0          Q02    2023.0   \n",
      "2        S      2017             NaN      1987.0          Q01    2023.0   \n",
      "3        S         -             NaN      1988.0          Q01    2023.0   \n",
      "4        S         -             NaN      1987.0          Q02    2023.0   \n",
      "\n",
      "  end_period  \n",
      "0        Q03  \n",
      "1        Q03  \n",
      "2        Q03  \n",
      "3        Q03  \n",
      "4        Q03  \n",
      "\n",
      "\n",
      "Dataset: pr_seasonal.csv\n",
      "  Seasonal_code            Seasonal_text\n",
      "0             S        Seasonal Adjusted\n",
      "1             U  Not Seasonally Adjusted\n",
      "2           NaN                      NaN\n",
      "\n",
      "\n",
      "Dataset: pr_measure.csv\n",
      "   measure_code             measure_text  display_level selectable  \\\n",
      "0           1.0               Employment            0.0          T   \n",
      "1           2.0     Average weekly hours            0.0          T   \n",
      "2           3.0             Hours worked            0.0          T   \n",
      "3           4.0  Real value-added output            0.0          T   \n",
      "4           5.0       Value-added output            0.0          T   \n",
      "\n",
      "   sort_sequence  \n",
      "0           11.0  \n",
      "1           12.0  \n",
      "2            4.0  \n",
      "3            2.0  \n",
      "4           15.0  \n",
      "\n",
      "\n",
      "Dataset: pr_data_0_Current.csv\n",
      "   series_id            year period         value footnote_codes\n",
      "0  PRS30006011        1995.0    Q01           2.6            NaN\n",
      "1  PRS30006011        1995.0    Q02           2.1            NaN\n",
      "2  PRS30006011        1995.0    Q03           0.9            NaN\n",
      "3  PRS30006011        1995.0    Q04           0.1            NaN\n",
      "4  PRS30006011        1995.0    Q05           1.4            NaN\n",
      "\n",
      "\n",
      "Dataset: pr_txt.csv\n",
      "                  Major Sector Productivity and Costs (PR)\n",
      "0                                             pr.txt      \n",
      "1                                    Section Listing      \n",
      "2                               1. Survey Definition      \n",
      "3       2. FTP files listed in the survey directory.      \n",
      "4  3. Time series, series file, data file, & mapp...      \n",
      "\n",
      "\n",
      "Dataset: pr_sector.csv\n",
      "   sector_code                      sector_name  display_level selectable  \\\n",
      "0       3000.0                    Manufacturing            0.0          T   \n",
      "1       3100.0     Manufacturing, Durable Goods            1.0          T   \n",
      "2       3200.0  Manufacturing, Nondurable Goods            1.0          T   \n",
      "3       8400.0                         Business            0.0          T   \n",
      "4       8500.0                 Nonfarm Business            0.0          T   \n",
      "\n",
      "   sort_sequence  \n",
      "0            4.0  \n",
      "1            5.0  \n",
      "2            6.0  \n",
      "3            2.0  \n",
      "4            1.0  \n",
      "\n",
      "\n",
      "Dataset: pr_contacts.csv\n",
      "                               Productivity Contacts\n",
      "0  Data Requests and Other Information: (202) 691...\n",
      "1  Analysts available from 8:30 a.m. to 4:30 p.m....\n",
      "2                       E-mail: productivity@bls.gov\n",
      "3                                News Media Contacts\n",
      "4  To speak directly with the BLS Press Office st...\n",
      "\n",
      "\n",
      "Dataset: pr_duration.csv\n",
      "   duration_code                     duration_text  display_level selectable  \\\n",
      "0            1.0  % Change same quarter 1 year ago            0.0          T   \n",
      "1            2.0    % Change from previous quarter            0.0          T   \n",
      "2            3.0                  Index (2017=100)            0.0          T   \n",
      "3            NaN                               NaN            NaN        NaN   \n",
      "\n",
      "   sort_sequence  \n",
      "0            1.0  \n",
      "1            2.0  \n",
      "2            3.0  \n",
      "3            NaN  \n",
      "\n",
      "\n",
      "Dataset: pr_data_1_AllData.csv\n",
      "   series_id            year period         value footnote_codes\n",
      "0  PRS30006011        1988.0    Q01           1.9            NaN\n",
      "1  PRS30006011        1988.0    Q02           2.2            NaN\n",
      "2  PRS30006011        1988.0    Q03           1.9            NaN\n",
      "3  PRS30006011        1988.0    Q04           1.1            NaN\n",
      "4  PRS30006011        1988.0    Q05           1.8            NaN\n",
      "\n",
      "\n",
      "Dataset: pr_class.csv\n",
      "   class_code   class_text  display_level selectable  sort_sequence\n",
      "0         3.0    Employees            0.0          T            2.0\n",
      "1         6.0  All workers            0.0          T            1.0\n",
      "2         NaN          NaN            NaN        NaN            NaN\n",
      "\n",
      "\n",
      "Dataset: pr_period.csv\n",
      "  period period_abbr     period_name\n",
      "0    Q01        QTR1     1st Quarter\n",
      "1    Q02        QTR2     2nd Quarter\n",
      "2    Q03        QTR3     3rd Quarter\n",
      "3    Q04        QTR4     4th Quarter\n",
      "4    Q05       AN AV  Annual Average\n",
      "\n",
      "\n",
      "Dataset: pr_footnote.csv\n",
      "  footnote_code footnote_text\n",
      "0             R       revised\n",
      "1           NaN           NaN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the list of CSV files in the \"data\" directory\n",
    "csv_files = [file for file in os.listdir(\"data\") if file.endswith(\".csv\")]\n",
    "\n",
    "# Read each CSV file and display its contents\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(\"data\", file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset: {file}\")\n",
    "    print(df.head())  # Display the first few rows of the dataset\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.storage_class import S3Wrapper\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# S3 bucket and landing zone details\n",
    "bucket_name = 'rearc-datalake-bucket'\n",
    "landing_zone_prefix = 'landing-zone/'\n",
    "\n",
    "# Create an instance of the s3wrapper class\n",
    "s3 = S3Wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'S3Wrapper' object has no attribute 'list_buckets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_buckets\u001b[49m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuckets\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m      4\u001b[0m     buckets \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuckets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'S3Wrapper' object has no attribute 'list_buckets'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
