{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: AWS S3 & Sourcing Datasets\n",
    "Republish this open dataset in Amazon S3 and share with us a link.\n",
    "You may run into 403 Forbidden errors as you test accessing this data. There is a way to comply with the BLS data access policies and re-gain access to fetch this data programatically - we have included some hints as to how to do this at the bottom of this README in the Q/A section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required sub-steps to accomplisht this:\n",
    "1. Set up S3 bucket environmnet\n",
    "2. Be able to read and publish into S3 env. \n",
    "3. troubleshoot error\n",
    "\n",
    "- Key notes: This is DataLake design. Create a landing-zone where data will be uploaded and parsed under upload date. The goal is just to have a starting point to being bringing in data into s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = os.getenv('AWS_ACCESS_KEY')\n",
    "secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-02-ep-website\n",
      "aws-emr-resources-386175835981-us-east-2\n",
      "aws-logs-386175835981-us-east-2\n",
      "bigbatbucket\n",
      "canvas-bucket-iris-4123\n",
      "dbdatalocation\n",
      "ed-exp-cost-and-usage\n",
      "eddysfistbuck\n",
      "edwardplatagschoolcap\n",
      "eplatacapstonedata\n",
      "eplatacapstoneipynb\n",
      "qep-sports-betting-bucket\n",
      "rearc-datalake-bucket\n",
      "redditdatacollectionwemeta\n",
      "sagemaker-soln-ddf-js-2ruwg4-386175835981-us-east-1\n",
      "sagemaker-soln-ddf-js-2seloa-386175835981-us-east-1\n",
      "sagemaker-soln-ddf-js-2sf3s6-386175835981-us-east-1\n",
      "sagemaker-soln-ddf-js-44xdya-386175835981-us-east-1\n",
      "sagemaker-soln-documents-js-4htc2a-us-east-1-386175835981\n",
      "sagemaker-studio-386175835981-l4ayz3cscdq\n",
      "sagemaker-studio-386175835981-l9tzph12na\n",
      "sagemaker-studio-386175835981-zzqac2052o\n",
      "sagemaker-us-east-2-386175835981\n",
      "someonesbucket\n",
      "ss-discord-group-minecraft-bucket\n"
     ]
    }
   ],
   "source": [
    "response = s3_client.list_buckets()\n",
    "\n",
    "if 'Buckets' in response:\n",
    "    buckets = response['Buckets']\n",
    "    for bucket in buckets:\n",
    "        print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have access to the s3 bucekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: AWS S3 & Sourcing Datasets\n",
    "1. Republish [this open dataset](https://download.bls.gov/pub/time.series/pr/) in Amazon S3 and share with us a link.\n",
    "    - You may run into 403 Forbidden errors as you test accessing this data. There is a way to comply with the BLS data access policies and re-gain access to fetch this data programatically - we have included some hints as to how to do this at the bottom of this README in the Q/A section.\n",
    "2. Script this process so the files in the S3 bucket are kept in sync with the source when data on the website is updated, added, or deleted.\n",
    "    - Don't rely on hard coded names - the script should be able to handle added or removed files.\n",
    "    - Ensure the script doesn't upload the same file more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "\n",
    "# AWS credentials\n",
    "access_key = os.getenv('AWS_ACCESS_KEY')\n",
    "secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# S3 bucket and landing zone details\n",
    "bucket_name = 'rearc-datalake-bucket'\n",
    "landing_zone_prefix = 'landing-zone/'\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")\n",
    "\n",
    "# Check if the landing zone exists, create it if it doesn't\n",
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=landing_zone_prefix\n",
    ")\n",
    "\n",
    "if 'Contents' not in response:\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=landing_zone_prefix\n",
    "    )\n",
    "    print(f\"Landing zone '{landing_zone_prefix}' created in bucket '{bucket_name}'.\")\n",
    "\n",
    "# Fetch data from the provided link and upload to S3 landing zone\n",
    "url = 'https://download.bls.gov/pub/time.series/pr/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    files = response.text.split('\\n')\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            file_name = file.split('/')[-1]\n",
    "            s3_client.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=f\"{landing_zone_prefix}{file_name}\",\n",
    "                Body=requests.get(f\"{url}{file}\").content\n",
    "            )\n",
    "            print(f\"Uploaded '{file_name}' to landing zone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://download.bls.gov/pub/time.series/pr/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    files = response.text.split('\\n')\n",
    "    for file in files:\n",
    "        print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'update_date': '12/6/2023',\n",
       "  'update_time': '8:30 AM',\n",
       "  'size': '102',\n",
       "  'data': 'pr.class 9/13/2022  3:52 PM          562 pr.contacts 12/6/2023  8:30 AM      1477682 pr.data.0.Current 12/6/2023  8:30 AM      3101899 pr.data.1.AllData 12/6/2023  8:30 AM          176 pr.duration 12/6/2023  8:30 AM           40 pr.footnote 12/6/2023  8:30 AM          745 pr.measure  1/7/1994  2:53 PM          146 pr.period11/18/2011  3:05 PM           79 pr.seasonal 12/6/2023  8:30 AM          263 pr.sector 12/6/2023  8:30 AM        15657 pr.series11/17/2011  4:11 PM        18343 pr.txt',\n",
       "  'data_link': 'https://download.bls.gov/pub/time.series/pr/pr.class 9/13/2022  3:52 PM          562 pr.contacts 12/6/2023  8:30 AM      1477682 pr.data.0.Current 12/6/2023  8:30 AM      3101899 pr.data.1.AllData 12/6/2023  8:30 AM          176 pr.duration 12/6/2023  8:30 AM           40 pr.footnote 12/6/2023  8:30 AM          745 pr.measure  1/7/1994  2:53 PM          146 pr.period11/18/2011  3:05 PM           79 pr.seasonal 12/6/2023  8:30 AM          263 pr.sector 12/6/2023  8:30 AM        15657 pr.series11/17/2011  4:11 PM        18343 pr.txt'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "def fetch_front_page_data(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    # Fetch the content from the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data from {url} with status code {response.status_code}\")\n",
    "\n",
    "    # Parse the HTML to extract file details\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    file_data = []\n",
    "\n",
    "    # Regular expression to match the file details\n",
    "    file_info_pattern = re.compile(r'(\\d{1,2}/\\d{1,2}/\\d{4})\\s+(\\d{1,2}:\\d{2} [APM]{2})\\s+([\\d,]+) (.+)')\n",
    "\n",
    "    for line in soup.get_text().split('\\n'):\n",
    "        match = file_info_pattern.search(line)\n",
    "        if match:\n",
    "            data = {\n",
    "                'update_date': match.group(1),\n",
    "                'update_time': match.group(2),\n",
    "                'size': match.group(3),\n",
    "                'data': match.group(4),\n",
    "                'data_link': os.path.join(url, match.group(4))\n",
    "            }\n",
    "            file_data.append(data)\n",
    "\n",
    "    return file_data\n",
    "\n",
    "# Example usage\n",
    "front_page_data = fetch_front_page_data(\"https://download.bls.gov/pub/time.series/pr/\")\n",
    "front_page_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>update_date</th>\n",
       "      <th>update_time</th>\n",
       "      <th>size</th>\n",
       "      <th>data</th>\n",
       "      <th>data_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/6/2023</td>\n",
       "      <td>8:30 AM</td>\n",
       "      <td>102</td>\n",
       "      <td>pr.class 9/13/2022  3:52 PM          562 pr.co...</td>\n",
       "      <td>https://download.bls.gov/pub/time.series/pr/pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  update_date update_time size  \\\n",
       "0   12/6/2023     8:30 AM  102   \n",
       "\n",
       "                                                data  \\\n",
       "0  pr.class 9/13/2022  3:52 PM          562 pr.co...   \n",
       "\n",
       "                                           data_link  \n",
       "0  https://download.bls.gov/pub/time.series/pr/pr...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(front_page_data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'next_sibling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(file_data)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Fetching and displaying data in DataFrame format\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m front_page_data_df \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_front_page_data_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://download.bls.gov/pub/time.series/pr/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m front_page_data_df\u001b[38;5;241m.\u001b[39mhead()  \u001b[38;5;66;03m# Displaying first few rows for brevity\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mfetch_front_page_data_df\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Extract each file entry\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     line_text \u001b[38;5;241m=\u001b[39m \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_previous\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_sibling\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line_text:\n\u001b[1;32m     21\u001b[0m         parts \u001b[38;5;241m=\u001b[39m line_text\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'next_sibling'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_front_page_data_df(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    # Fetch the content from the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data from {url} with status code {response.status_code}\")\n",
    "\n",
    "    # Parse the HTML to extract file details\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    file_data = []\n",
    "\n",
    "    # Extract each file entry\n",
    "    for line in soup.find_all('a'):\n",
    "        line_text = line.find_previous('br').next_sibling.strip()\n",
    "        if line_text:\n",
    "            parts = line_text.split()\n",
    "            if len(parts) >= 3:\n",
    "                update_date = parts[0]\n",
    "                update_time = parts[1] + ' ' + parts[2]\n",
    "                size = parts[3]\n",
    "                data = line.text\n",
    "                data_link = url + line['href']\n",
    "\n",
    "                file_info = {\n",
    "                    'update_date': update_date,\n",
    "                    'update_time': update_time,\n",
    "                    'size': size,\n",
    "                    'data': data,\n",
    "                    'data_link': data_link\n",
    "                }\n",
    "                file_data.append(file_info)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(file_data)\n",
    "\n",
    "# Fetching and displaying data in DataFrame format\n",
    "front_page_data_df = fetch_front_page_data_df(\"https://download.bls.gov/pub/time.series/pr/\")\n",
    "front_page_data_df.head()  # Displaying first few rows for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_front_page_data_df(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data from {url} with status code {response.status_code}\")\n",
    "\n",
    "    # Save the HTML content\n",
    "    today = datetime.today().strftime('%Y_%m_%d')\n",
    "    filename = f'downloaded_data/front_page_{today}.html'\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = {}  # replace this with your parsing logic\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'downloaded_data/front_page_2023_12_16.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.bls.gov/pub/time.series/pr/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_front_page_data_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mfetch_front_page_data_df\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     16\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownloaded_data/front_page_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     19\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Parse the HTML content\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rearc-env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'downloaded_data/front_page_2023_12_16.html'"
     ]
    }
   ],
   "source": [
    "url = \"https://download.bls.gov/pub/time.series/pr/\"\n",
    "df = fetch_front_page_data_df(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved local\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_front_page(url, download_dir='downloaded_data', retries=3):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    for i in range(retries):\n",
    "        # Fetch the content from the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            # Create the download directory if it doesn't exist\n",
    "            os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "            # Save the HTML content to a file\n",
    "            date_str = datetime.now().strftime('%Y%m%d')\n",
    "            file_path = os.path.join(download_dir, f'frontpage_{date_str}.html')\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "\n",
    "            return file_path\n",
    "        else:\n",
    "            time.sleep(2)  # Wait for 2 seconds before retrying\n",
    "\n",
    "    raise Exception(f\"Failed to fetch data from {url} after {retries} attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_front_page(file_path):\n",
    "    # Read the HTML content from the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # Parse the HTML to extract file details\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    file_data = []\n",
    "\n",
    "    # Regular expression to match the file details\n",
    "    file_info_pattern = re.compile(r'(\\d{1,2}/\\d{1,2}/\\d{4})\\s+(\\d{1,2}:\\d{2} [APM]{2})\\s+([\\d,]+) (.+)')\n",
    "\n",
    "    for line in soup.get_text().split('\\n'):\n",
    "        match = file_info_pattern.search(line)\n",
    "        if match:\n",
    "            data = {\n",
    "                'update_date': match.group(1),\n",
    "                'update_time': match.group(2),\n",
    "                'size': match.group(3),\n",
    "                'data': match.group(4),\n",
    "                'data_link': os.path.join(url, match.group(4))\n",
    "            }\n",
    "            file_data.append(data)\n",
    "\n",
    "    return file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Failed to fetch data from https://download.bls.gov/pub/time.series/pr/ after 3 attempts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_front_page\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://download.bls.gov/pub/time.series/pr/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m, in \u001b[0;36mdownload_front_page\u001b[0;34m(url, download_dir, retries)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Wait for 2 seconds before retrying\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m attempts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Failed to fetch data from https://download.bls.gov/pub/time.series/pr/ after 3 attempts"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_path = download_front_page(\"https://download.bls.gov/pub/time.series/pr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_page_data = parse_front_page(file_path)\n",
    "front_page_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
